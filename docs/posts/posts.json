[
  {
    "path": "posts/2021-05-22-googles-tensor-processing-unit/",
    "title": "Google's Tensor Processing Unit",
    "description": "Just my incoherent thoughts on Google's TPUs.",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [],
    "contents": "\nTheoretical Background\nGPUs originated from the gaming community because a CPU could not handle specialised tasks such as rasterisation, texture mapping, frame buffer operations etc Harris (2005). The deep learning community adopted GPUs and has continued to use them and extend their capabilities(Navarro et al. 2020).\nHowever, further performance gains could not be extracted via the addition of transistors alone. As a more spearheaded approach, TPUs were conceived with the idea(N. Jouppi et al. 2018) that removing graphics capabilities and focusing on matrix multiplication capabilities could lead to better performance and power efficiency.\n\nAs is the case with a lot of new advancements, the theoretical component needed to make this leap already existed. Work on systolic matrix multiplication had existed since the 1980s and it was put to use in TPUs.\nThree versions of TPUs have been released with v2 and v3 being accessible to the general public via Google Cloud.\nSystolic Architectures\nSystolic architecture design is a general methodology for mapping high level computations into hardware structures developed at Carnegie Melon University around 1980(Kung 1982).\n\nSystolic architecture gets its name from analogising the human heart, which pumps oxygenated blood throughout the body and receives de-oxygenated blood back from the organs with a computer’s memory, which pumps data throughout the processing units and gets processed data back.\nWhen the number of operations is greater than the total number of I/O elements, the task is said to be “Compute Bound”. In the reverse case, the task is said to be “I/O bound”. As an example, matrix multiplication is a compute bound task while matrix addition is an I/O bound task.(Williams, Waterman, and Patterson 2009)\n\nSystolic architectures have found applications in various mathematical computations (Milovanović et al. 2010) but they can only speed up tasks that are compute bound.\nEvolution of TPUs\nTPUv1\nGoogle’s TPU v1 was put into production in 2015 and it was used internally by Google for their applications. In 2017 Google finally published a technical description of the chip called “In-Datacenter Performance Analysis of a Tensor Processing Unit” (N. P. Jouppi et al. 2017).\n\nTPUv1 was designed as a coprocessor with a PCIe Gen3 x16 bus ( 12GB/s bandwidth). Similar to a GPU, it was designed to work with existing infrastructure and one server could be connected to 4 TPUs. A TPU is more like an FPU (Floating Point Unit) than a GPU because the host sends the instructions to the TPU whereas a GPU fetches the instructions on its own.\nThe central piece of the TPU is the MMU (Matrix Multiply Unit). This is the component that is based on systolic architectures. It contains 256x256 MAC (multiply and accumulate) units that can perform 8-bit multiplications and additions on signed or unsigned integers. This means that:\n\\[T = 256*256\\ ops * 700MHz\\] \\[= 45.2 T ops/s\\] These are just multiply operations. So, adding the addition operations, \\[T = 45.2 * 2 \\approx 90 T ops/s\\]\nThe weights for the matrix unit are staged through an on-chip Weight FIFO that reads from an off-chip 8 GB DRAM called Weight Memory (two 2133MHz DDR3 DRAM channels) for inference, weights are read-only. 8 GB supports many simultaneously active models.\n\nThe 16-bit products are collected in the 4 MB of 32-bit Accumulators below the matrix unit. The 4MB represents 4096, 256-element, 32-bit accumulators.\n\nThe matrix unit produces one 256-element partial sum per clock cycle. When using a mix of 8-bit weights and 16-bit activations (or vice versa), the Matrix Unit computes at half-speed, and it computes at a quarter-speed when both are 16 bits.The intermediate results are held in the 24 MB on-chip Unified Buffer, which can serve as inputs to the Matrix Unit. A programmable DMA controller transfers data to or from CPU Host memory and the Unified Buffer. The 24 MB Unified Buffer is almost a third of the die and the Matrix Multiply Unit is a quarter.(Sato, Young, and Patterson 2017)\n\nThe TPU ASIC is built on a 28nm process and consumes 40W when running (75W TDP). TPU implements the matrix multiplication with the systolic array in a pipeline fashion. It relies on data from different directions arriving at cells in an array at regular intervals and being combined.\nThe TPUv1 SchematicTPUv2\nTPU v2 was unveiled at Google I/O in May 2017, two years later. While TPU v1 is a coprocessor, controlled by the host, TPU v2 and successors are Turing-complete and are suitable for both training and inference. Importantly, TPU v2 was built for multi-chip configurations because it became critical due to heavy production workloads — a single TPU v2 would take 60–400 days for some of them.\n\nA “Cloud TPU” is a TPU board with 4 TPU chips connected through PCI to a host virtual machine.\n\nEach TPU chip contains two cores with 8 GiB of HBM (high-bandwidth memory) and one matrix unit (MXU) for each TPU core. TPU v2 is connected to the host by PCIe Gen3 x32. The MXU provides the bulk of computing power in a TPU chip. Each MXU is capable of performing 16K (128x128) multiply-accumulate operations in each cycle at reduced bfloat16(Wang 2019)(Kalamkar et al. 2019) (BF16) precision. It is supported by a vector processing unit that performs all the other computations in typical training workloads.(Patterson 2018)\n\nMatrix multiplications are performed with BF16 inputs but all accumulations happen in FP32 so the resulting matrix is FP32(Courbariaux, Bengio, and David 2014). All other computations are in FP32 except for results going directly to an MXU input, which are converted to BF16.\nThe Scalar Unit fetches VLIW(Fisher 1984) (Very Long Instruction Word) instructions from the core’s on-chip, software-managed Instruction Memory (Imem), executes scalar operations using a 4K 32-bit scalar data memory (Smem) and 32 32-bit scalar registers (Sregs), and forwards vector instructions to the Vector Unit. The 322-bit VLIW instruction can launch eight operations: two scalar, two vector ALU, vector load and store, and a pair of slots that queue data to and from the matrix multiply and transpose units. The XLA compiler(PyTorch 2020) schedules loading Imem via independent overlays of code, as unlike conventional CPUs, there is no instruction cache.\n\nThe Vector Unit performs vector operations using a large on-chip vector memory (Vmem) with 32K 128 x 32-bit elements (16MB), and 32 2D vector registers (Vregs) each containing 128 x 8 32-bit elements (4 KB). The Vector Unit streams data to and from the MXU through decoupling FIFOs. The Vector Unit collects and distributes data to Vmem via data-level parallelism (2D matrix and vector functional units) and instruction- level parallelism (8 operations per instruction).\n\nThe Transpose, Reduction, Permute Unit performs efficient common matrix transformations. ICI enables direct connections between chips (500 GB/s per link) to form a supercomputer using only 13% of each chip. Direct links simplify rack-level deployment, but in a multi-rack system, the racks must be adjacent. TPU v1 was memory bound for most of its applications. Engineers solved its memory bottleneck by using High Bandwidth Memory (HBM) DRAM in TPU v2 with 700 GB/s bandwidth instead of 34 GB/s bandwidth of DDR3 in TPU v1. More on memory speed here. TPU v2 delivers 22.5 TFLOPS per core, so 45 TFLOPS per chip and 180 TFLOPS per Cloud TPU card(Shahid and Mushtaq 2020).\n\nTPUv3\nTPU v3 was announced in May 2018 at Google I/O, only a year after the TPU v2. It’s rather a gradual evolution of TPU v2, or maybe “TPU v2 done right” (in one deck Google called it “The Anti-Second System”).\nA “Cloud TPU v3” is still a TPU board with 4 TPU chips. Each TPU v3 chip contains two cores with 16 GB of HBM (increased size twice and bandwidth from 700 to 900 GB/s) and two matrix units (MXU) for each TPU core (instead of a single one in TPU v2). Now it is liquid-cooled (so the tubes on the photo).\nTPU v3 delivers 420 TFLOPS per Cloud TPU card (due to twice the number of MXU and increased clock rate (940 MHz vs 700 MHz)(Teich 2018).\nBenchmarking TPUs\nMethodology\nThe main idea is to benchmark individual operations of the TPU. The question then remains what operations should be benchmarked. The operations are divided into 4 categories:\nBasic Functions (Add, subtract, multiply, divide, power, sqrt, abs, log, exp)\nActivation Functions (ReLU, Leaky ReLU, sign, sigmoid, softplus, relu6, hardswish, mish, log-softmax, tanh)\nMatrix Multiplication (dot product, 2-D dot product, 3-D dot product, 4-D dot product, 4-D sum, Matrix Multiplication)\nOther Matrix Operations (Scalar multiplication, reverse scalar multiplication, subtract, reverse subtract)\nFor each operation, a function stress_test is called that first generates a matrix of a randomly selected integer size between size \\(2^i\\) and \\(2^{i+1}\\) and \\(i\\) is increased until the TPU/GPU cannot handle the computation (A GPU throws a CUDA out of memory error while a TPU crashes). The decision to select a random integer that’s not an exact power of 2 is based on the fact that these devices are optimized for sizes that are a power of 2. To get rid of this optimization factor and test raw power, the size is random and not a perfect power of 2.\nProblems Encountered\nIn order to compare TPUs and GPUs, I needed to rent a GPU in the cloud. However, Google was and still is refusing to grant my request for GPUs. I had to use GPUs available in the cloud and that presented a problem because Google Cloud run on a shared memory architecture. This means that the amount of memory in a system is not fixed. It varies depending on the current load on the system. This is problematic for benchmarking because the conditions cannot be controlled. I’ll update this document if the situation changes.\nResults\nOn GitHub.\n\n\n\nCourbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. 2014. “Training deep neural networks with low precision multiplications.” 3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings, no. Section 5 (December): 1–10. http://arxiv.org/abs/1412.7024.\n\n\nFisher, Joseph A. 1984. “The VLIW Machine: A Multiprocessor for Compiling Scientific Code.”\n\n\nHarris, Mark. 2005. “Mapping Computational Concepts to GPUs Chapter 31 Excerpted from GPU Gems 2.”\n\n\nJouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. “In-Datacenter Performance Analysis of a Tensor Processing Unit.” ACM SIGARCH Computer Architecture News 45 (2): 1–12. https://doi.org/10.1145/3140659.3080246.\n\n\nJouppi, Norman, Cliff Young, Nishant Patil, and David Patterson. 2018. “Motivation for and Evaluation of the First Tensor Processing Unit.” IEEE Micro 38 (3): 10–19. https://doi.org/10.1109/MM.2018.032271057.\n\n\nKalamkar, Dhiraj, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, et al. 2019. “A Study of BFLOAT16 for Deep Learning Training,” June. http://arxiv.org/abs/1905.12322.\n\n\nKung. 1982. “Why systolic architectures?” Computer 15 (1): 37–46. https://doi.org/10.1109/MC.1982.1653825.\n\n\nMilovanović, I. Ž., M P Bekakos, I N Tselepis, and E. I. Milovanović. 2010. “Forty-three ways of systolic matrix multiplication.” International Journal of Computer Mathematics 87 (6): 1264–76. https://doi.org/10.1080/00207160802275944.\n\n\nNavarro, Cristóbal A, Roberto Carrasco, Ricardo J Barrientos, Javier A Riquelme, and Raimundo Vega. 2020. “GPU Tensor Cores for fast Arithmetic Reductions,” June. http://arxiv.org/abs/2001.05585.\n\n\nPatterson, David. 2018. “50 Years of computer architecture: From the mainframe CPU to the domain-specific tpu and the open RISC-V instruction set.” In 2018 IEEE International Solid - State Circuits Conference - (ISSCC), 27–31. IEEE. https://doi.org/10.1109/ISSCC.2018.8310168.\n\n\nPyTorch. 2020. “PyTorch XLA.” https://github.com/pytorch/xla.\n\n\nSato, Kaz, Cliff Young, and David Patterson. 2017. “An in-depth look at Google’s first Tensor Processing Unit (TPU) | Google Cloud Blog.”\n\n\nShahid, Amna, and Malaika Mushtaq. 2020. “A Survey Comparing Specialized Hardware and Evolution in TPUs for Neural Networks.” In Proceedings - 2020 23rd IEEE International Multi-Topic Conference, INMIC 2020. Institute of Electrical; Electronics Engineers Inc. https://doi.org/10.1109/INMIC50486.2020.9318136.\n\n\nTeich, Paul. 2018. “Tearing Apart Google ’ s TPU 3 . 0 AI Coprocessor.” https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/.\n\n\nWang, Shibo. 2019. “BFloat16: The secret to high performance on Cloud TPUs.” https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus.\n\n\nWilliams, Samuel, Andrew Waterman, and David Patterson. 2009. “Roofline: An insightful visual performance model for multicore architectures.” Communications of the ACM 52 (4): 65–76. https://doi.org/10.1145/1498765.1498785.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-25T16:34:31+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-notes-on-david-mackays-information-theory-inference-and-learning-algorithms/",
    "title": "Notes on David MacKay's Information Theory, Inference and Learning Algorithms",
    "description": "I recently started reading David MacKay's book and this blog post will contain my notes and thoughts on the book. I plan on using Mathematica for basic plotting and calculations.",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-03-20",
    "categories": [],
    "contents": "\nOkay, let’s start with the basics.\nWhy are you using Mathematica?\nI was a MATLAB person, primarily because that’s what is used at our college. I got very comfortable with MATLAB and until a couple of months ago, I didn’t have a preference.\nHowever, when I started focusing on the mathematical aspects, MATLAB became a huge pain. Yes, MATLAB has notebooks. Yes, MATLAB has markdown support. Yes, MATLAB is very efficient because it’s built on top of FORTRAN. Yes, MATLAB has a huge user base. But here’s where the problem begins. MATLAB did not start out this way. It didn’t really have notebooks and stuff. Over the years, MATLAB has gotten INCREDIBLY messy. I mean, the syntax is so clunky, it makes my blood boil.\nSo, I use Mathematica for experimenting with stuff. It’s really quick and the plotting works flawlessly. The only issue I have is that it doesn’t work natively on my M1 Mac. It’s a little annoying but, I can learn to live with it. # Chapter 2\nLink to notes: https://www.icloud.com/iclouddrive/0uR4KHxhwZgekHbSlGJLbGzrA#Chapter2_MacKay\nChapter 4\nIntroduction\nMeasuring Information\nShannon’s Information Content \\[h(x=a_i) = log_2(\\frac{1}{P(x)})\\]\nThe entropy of an ensemble \\(X\\) is given by \\[H(x) = \\sum_{x \\epsilon X}P(x)log_2(\\frac{1}{P(x)})\\]\nNow, where did this \\(log\\) come from? Why does the above measure make sense? Intuitively, it’s because \\(log\\) has a couple of really nice properties:\nSay \\(P(X,Y) = P(X)P(Y)\\) i.e. X and Y are independent. The information we receive on the occurrence of both X and Y should be the sum of the information received on X and Y occurring. \\[\\begin{aligned}\n        log(\\frac{1}{P(X,Y)}) & = log(\\frac{1}{P(X)}) + log(\\frac{1}{P(Y)})\\\\\n        h(x,y)& = h(x) + h(y)\n    \\end{aligned}\\]\nWhat about the entropy of two independent events. We know that \\(H(X)=\\sum p_i log(\\frac{1}{p_i})\\). Let’s see how that extends to the joint entropy.\n\\[\\begin{aligned}\n        H(X,Y) & = \\sum_x \\sum_y P(x)P(y)log(\\frac{1}{P(x)P(y)}) \\\\\n                &= \\sum_x \\sum_y P(x)P(y)(log(\\frac{1}{P(x)}) + log(\\frac{1}{P(y)})) \\\\\n                &= \\sum_x P(x)log(\\frac{1}{P(x)})\\sum_y P(y) + \\sum_y P(y)log(\\frac{1}{P(y)})\\sum_x P(x) \\\\\n                &= H(X) + H(Y)\n    \\end{aligned}\\] So, the logarithm does lend the information measure a few tricks.\nThe Weighing Problem\nProblem Statement\n\nYou are given 12 balls, all equal in weight except for one that is either heavier or lighter. You are also given a two-pan balance to use. In each use of the balance you may put any number of the 12 balls on the left pan, and the same number on the right pan, and push a button to initiate the weighing; there are three possible outcomes: either the weights are equal, or the balls on the left are heavier, or the balls on the left are lighter. Your task is to design a strategy to determine which is the odd ball and whether it is heavier or lighter than the others in as few uses of the balance as possible.\n\nFun fact: This problem was featured on Brooklyn Nine-Nine and I had been trying to solve it for a while. I found the solution! Who knew it was in information theory? The form of the problem is slightly different. It’s islanders instead of balls. In case you’re wondering which episode this was, it was S02E18.\nThe key to solving this problem is realising that the information gain is maximised when there are as many equiprobable outcomes as possible.\nDon’t get it? Don’t worry, here the math:\nWe have, \\[\\sum_x P(x)log(\\frac{1}{P(x)}) = H(X)\\]\nFrom Jensen’s inequality, we know: \\[E[f(x)] \\leq f(E[x])\\] If we take \\(f(x)=log(x)\\) \\[\\begin{aligned}\n    E[f(\\frac{1}{P(x)})] & \\leq f(E[\\frac{1}{P(x)}]) \\\\\n    \\sum_x P(x)log(\\frac{1}{P(x)}) & \\leq f(|A_x|) \\\\\n    H(X) & \\leq log(|A_x|)\\end{aligned}\\]\nThe equality holds only when all the outcomes are equiprobable.\nSo, the best strategy is dividing it up into equiprobable outcomes. Even in Brooklyn Nine Nine, when Amy started with “we weight six against six…” Holt immediately stopped her and told her that it would never work.\n\nHere’s a description of the optimal strategy in case the diagram seems confusing:\nWeigh 1234 against 5678 and leave out 4 balls. This divides it into three equiprobable outcomes. The first outcome is when the left is heavier, the second one is when the right is heavier and the third one is when they are equal in weight. (Think of the weighing as a sieve and the hypotheses as impurities or whatever.)\nNow, we weigh three against three. I’ll explain for the first branch. Why do we choose 126 against 345? Actually, we could have chosen 125 against 346 or 127 against 348. If you can find any other weighing scheme that creates three equiprobable outcomes, it’s fine.\nThen in the third weighing, we weigh one against one. The star denotes an impossible outcome.\nNow, what gives me the right to call my strategy optimal? Well, we have to refine the original hypothesis space into a single hypothesis. If we weigh twice, then the final number of branches is \\(3^2=9\\) which is less than 24. Three weighings creates \\(3^3=27\\) branches which is a little over 24. So, it’s theoretically impossible to find the right hypothesis in less than three weighings if we assume that we’re dividing it into 3 equiprobable outcomes at every step. If you can find a strategy with 4 equiprobable outcomes per step, shoot me an email.\nThe Theoretical Limits of Compression\nRaw Bit Content\n\\[H_0 (x) = log_2 |A_x|\\]\nThe raw bit content gives a lower bound on the number od questions needed to find a binary answer. Again, the log comes in handy. Say we have an ordered pair \\((x,y)\\) with \\(|A_x||A_y|\\) outcomes,\n\\[\\begin{aligned}\n    H_0 (x,y) & = log_2 |A_x||A_y| \\\\\n            &= log_2 |A_x| + log_2 |A_y| \\\\\n            &= H_0 (x) + H_0 (y)    \\end{aligned}\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-25T15:37:48+05:30",
    "input_file": {}
  }
]
