[
  {
    "path": "posts/2021-05-22-prml-bishop/",
    "title": "PRML, Bishop",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-22T16:27:19+05:30",
    "input_file": "prml-bishop.knit.md"
  },
  {
    "path": "posts/2021-05-22-googles-tensor-processing-unit/",
    "title": "Google's Tensor Processing Unit",
    "description": "Just my incoherent thoughts on Google's TPUs. ****UNDER CONSTRUCTION****",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [],
    "contents": "\nGoogle introduced TPUs in 2016. Tensor Processing Units or TPUs are Application Speciﬁc Integrated Circuits (ASICs) that are optimized for low precision, high volume matrix computations. Low precision, high volume calculations are the bedrock of NN training and the most widespread use (as well as the originally intended use) of TPUs is to accelerate neural network training and inference.\nGPUs originated from the gaming community because a CPU could not handle specialised tasks such as rasterisation, texture mapping, frame buffer operations etc. The deep learning community adopted GPUs and has continued to use them and extend their capabilities. However, further performance gains could not be extracted via the addition of transistors alone. As a more spearheaded approach, TPUs were conceived with the idea that removing graphics capabilities and focusing on matrix multiplication capabilities could lead to better performance and power efﬁciency.\n\nAs is the case with a lot of new advancements, the theoretical component needed to make this leap already existed. Work on systolic matrix multiplication had existed since the 1980s and it was put to use in TPUs. Three versions of TPUs have been released with v2 and v3 being accessible to the general public via Google Cloud.\n\nBackground\nSystolic Architectures\nSystolic architecture design is a general methodology for mapping high level computations into hardware structures developed at Carnegie Melon University around 1980.\nSystolic architecture gets its name from analogising the human heart, which pumps oxygenated blood throughout the body and receives deoxygenated blood back from the organs with a computer’s memory, which pumps data throughout the processing units and gets processed data back.\nWhen the number of operations is greater than the total number of I/O elements, the task is said to be “Compute Bound”. In the reverse case, the task is said to be “I/O bound”.\nAs an example, matrix multiplication is a compute bound task while matrix addition is an I/O bound task.\nThe original paper on systolic architectures is fairly accessible. In case you’re interested, the paper’s titled “Why Systolic Architectures?” by H.T. Kung. (http://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf)\n\nSystolic architectures can only speed up tasks that are compute bound.\n\nRoofline Graphs\nJust read Patterson’s article on it.\nbfloat16\nThis is a simple one. Lower precision arithmetic is faster. So, Google created a datatype that it calls BFLOAT16 with 1 sign bit, 8 exponent bits and 7 significand precision bits. Surprisingly enough, Wikipedia was very informative on this.\nComparison\n\nThe “new” bfloat16\n\nThe Evolution of TPUs\nTPUv1\nThere’s a real deficit of good quality training chips. NVidia has a monopoly on these things. Seriously, try to find a chip that’s powerful, natively supported by Python and not made by NVidia and the whole thing starts to look a lot like a monopoly.\nNVidia V100 is $8,799. Holy crap! That’s a lot of money for a chip.\nArchitecture\n\nIt looks a little intimidating at first glance but there’s really only a few noteworthy things.\nBandwidth Gap - DDR3 -> MMU is about 30GiB/s and Systolic Data Setup->MMU is about 167GiB/s. That’s a huge gap. When we consider how expensive retreiving something from RAM is, we start to see why we need systolic architectures.\nMatrix Multiply Unit\nCommon Neural Network Layers\n\nTPU v1 is manufactured on a 28 nm process with a die size ≤ 331 mm2, clock speed of 700 MHz, 28 MB of on-chip memory, 4 MB of 32-bit accumulators, and a 256×256 systolic array of 8-bit multipliers. For this reason, we can get 700Mhz*65536 (multipliers) → 92 Tera operations/sec. TPU v1 has an 8 GB dual-channel 2133 MHz DDR3 SDRAM offering 34 GB/s of bandwidth. It has thermal design power of 28-40 Watts, which is certainly low consumption compared to GPUs and CPUs.\n\nI should point out that it’s ~90T inference ops/s. I’m more interested in training than inference but it’s insanely fast.\nTPUv2\nThen Google came out with TPUv2 in 2017? Google gave a presentation at the hotchips conference and the slides do a good job of how the TPUv2 evolved from 1st gen TPUs.\n\n\n\n\n\n\n\n\n\n\nNow, to see the components in a little more detail, here’s a little figure I created:\n\nSo, how fast is it?\nOriginally, I was going to perform all experiments on TPUv3 via Google Cloud but that is such a pain! So, I decided to buy Colab Pro, and set the compute to TPU and High-Ram. PyTorch was recently modified to work on XLA devices which out TPU is. I’m proficient in PyTorch and it was fun to create these benchmarks. I’m not presenting the results until I hear from AWS about my GPU. I want to run them in a tightly controlled environment.\nHere’s the GitHub Repo: https://github.com/dxgp/TPU_PyTorch.git\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-02T16:13:13+05:30",
    "input_file": "googles-tensor-processing-unit.knit.md"
  },
  {
    "path": "posts/2021-03-21-reading-list-2021/",
    "title": "Reading List 2021",
    "description": "Just a list of books that I plan on finishing by the end of 2021.",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-03-21",
    "categories": [],
    "contents": "\nMathematics\nI’ve spent a long time thinking about whether math is necessary for machine learning. Here’s my answer: absolutely. Here’s the most popular analogy I’ve heard:\n\nIt’s just like a car. To drive it, I don’t have to know how the parts work.\n\nThat’s just stupid. You’re not driving the car in this scenario. You’re the one who’s fucking designing it. To design it, you can get away with not knowing the laws of thermodynamics but not without knowing how the parts fit together. There is an exception to this, though. If you wanna design cars that look like this:\n\n\nYou go, man! You’re ready if that’s what you wanna do. No math required.\n\nIf you actually want to apply machine learning to problems that are genuinely interesting and make a contribution, you have to know the math and the underlying theory. You have to know the limits of machine learning. Contrary to popular belief, machine learning is not ACTUALLY magic. Neural networks are not the solution to everything. It’s problem dependent. This is from the point of view of a guy who wants to go into research. If that’s not your goal, you can get away with knowing a lot less math than this.\n\nUnless you’re comfortable with the math, you’re going to crap your pants while reading research papers. Also, math’s applicable EVERYWHERE. So, you don’t really lose anything.\n\nFor a demonstration of crapping your pants, take a gander at this paper1. If you can understand the math in there, you’re good to go. Here’s a little preview:\n\\[{\\sum_{t=1}^T  \\mathcal S_{P^{\\phi_t}} \\left[\\frac{\\partial L}{\\partial \\phi_{t}}\\right] \\sum_{i=1}^{t} \\big(\\prod_{j = i}^{t-1} \\frac{\\partial \\phi_{j+1}}{\\partial \\phi_j}\\big) \\mathcal S_{P^{\\phi_{i-1}}} \\left[\\frac{\\partial \\phi_i}  {\\partial  C_{i-1}}\\right]\\frac{\\partial  C_{i-1}}{\\partial \\theta}\\,.}\\]\nIntimidating, right?\nI’ll conclude this rant with my math reading list for 2021:\nAnalysis I and II by Terence Tao\nInformation Theory, Inference, and Learning Algorithms by David MacKay\nVisual Complex Analysis by Tristan Needham\nTAOCP - Vol. 1 and 2 (Yeah, it’s mostly math so I’ll put it here)\nWhy TAOCP? Because I either play the game on hard mode or not at all.\nFor the last book, there’s a great accompanying course on YT. I’ll add the link to it in a few days. This year is going to be math heavy and that’s a bit of a problem because people usually only look at your projects and I don’t really have projects yet. I’ve tried making a CV a few times but I come up empty. Hopefully, it won’t be empty by the end of this year.\nI did consider Rudin for Analysis but I liked Tao’s style better.\nMachine Learning\nI do need to apply all that math. Here’s my machine learning reading list. I should warn you that this is fairly intermediate because I read the basic stuff about a year ago.\nThe Elements of Statistical Learning by Hastie et al.\nDeep Learning by Ian Goodfellow\nPattern Recognition and Machine Learning by Bishop\nReinforcement Learning by Sutton and Barto\nOther Books that I Think Might be Worth Reading\nSometimes, I just can’t resist the temptation to dive into a completely different field. I have the attention span of a 10 year old. That said, here’s the books that I can’t wait to read:\nProgramming Quantum Computers by Johnston\nNonlinear Dynamics and Chaos by Strogatz\nConcluding Remarks\nMy reading list is fairly small. That’s because we have vastly different courses that are currently going on in our college and I do have assignments to complete and exams to take.\n\nhttps://arxiv.org/abs/2007.10412↩︎\n",
    "preview": "https://media.wired.com/photos/593252a2a31264584499416b/master/w_1600%252Cc_limit/the-homer-inline2.jpg",
    "last_modified": "2021-06-02T16:19:42+05:30",
    "input_file": "reading-list-2021.knit.md"
  },
  {
    "path": "posts/2021-03-20-notes-on-david-mackays-information-theory-inference-and-learning-algorithms/",
    "title": "Notes on David MacKay's Information Theory, Inference and Learning Algorithms",
    "description": "I recently started reading David MacKay's book and this blog post will contain my notes and thoughts on the book. I plan on using Mathematica for basic plotting and calculations.",
    "author": [
      {
        "name": "Gunjan Payal",
        "url": {}
      }
    ],
    "date": "2021-03-20",
    "categories": [],
    "contents": "\nOkay, let’s start with the basics.\nWhy on Earth are you using Mathematica?\nI’m usually a MATLAB person, primarily because that’s what is used at our college. I got very comfortable with MATLAB and until a couple of months ago, I didn’t have a preference.\nHowever, when I started focusing on the mathematical , MATLAB became a huge pain in my ass. Yes, MATLAB has notebooks. Yes, MATLAB has markdown support. Yes, MATLAB is very efficient because it’s built on top of FORTRAN. Yes, MATLAB has a huge user base. But here’s where the problem begins. MATLAB did not start out this way. It didn’t really have notebooks and stuff. Over the years, MATLAB has gotten INCREDIBLY messy. I mean, the syntax is so clunky, it makes my blood boil.\nHere’s a question that crosses my mind: who is MATLAB perfect for?\nWhat’s a use case where MATLAB wins, hands down? The answer was very simple in 2007. Anyone involved in data analytics or numerical computation would have used MATLAB. However, a lot has changed since 2007. All the cool kids are now using Python (pioneers are switching to Julia though) and the number of python libraries is absolutely gigantic. So, if I’m involved in data analytics, MATLAB is not the optimal choice anymore.\n\nPython came into the spotlight in 2007\nIf we take a look at the popularity of programming languages 1, python has witnessed immense growth (from ~4% in 2006 to ~30% in 2021). MATLAB on the other hand, has witnessed a decline (~3% in 2006 to ~2% in 2021). Now, here’s what people will say:\n\nGee, MATLAB may have witnessed a 1% decline but Wolfram didn’t even make the list. LOL.\n\nThat’s just plain stupid! Wolfram lies in a different category. MATLAB is marketed as: “Math. Graphics. Programming.” 2 MATLAB markets itself as:\n\nMATLAB is a programming and numeric computing platform used by millions of engineers and scientists to analyze data, develop algorithms, and create models.\n\nChapter 2\nLink to notes: https://www.icloud.com/iclouddrive/0uR4KHxhwZgekHbSlGJLbGzrA#Chapter2_MacKay\n\nhttps://pypl.github.io/PYPL.html↩︎\nhttps://www.mathworks.com/products/matlab.html↩︎\n",
    "preview": {},
    "last_modified": "2021-03-31T03:10:06+05:30",
    "input_file": {}
  }
]
