---
title: "Chapter 4: David MacKay's Information Theory"
description: |
  A short description of the post.
draft: true
author:
  - name: Gunjan Payal
    url: {}
date: 06-25-2021
output:
  distill::distill_article:
    self_contained: false
    
---
# Introduction

## Measuring Information

**Shannon's Information Content** $$h(x=a_i) = log_2(\frac{1}{P(x)})$$

The entropy of an ensemble $X$ is given by
$$H(x) = \sum_{x \epsilon X}P(x)log_2(\frac{1}{P(x)})$$

Now, where did this $log$ come from? Why does the above measure make
sense? Intuitively, it's because $log$ has a couple of really nice
properties:

1.  Say $P(X,Y) = P(X)P(Y)$ i.e. X and Y are independent. The
    information we receive on the occurrence of both X and Y should be
    the sum of the information received on X and Y occurring.
    $$\begin{aligned}
            log(\frac{1}{P(X,Y)}) & = log(\frac{1}{P(X)}) + log(\frac{1}{P(Y)})\\
            h(x,y)& = h(x) + h(y)
        \end{aligned}$$

2.  What about the entropy of two independent events. We know that
    $H(X)=\sum p_i log(\frac{1}{p_i})$. Let's see how that extends to
    the joint entropy.

3.  $$\begin{aligned}
            H(X,Y) & = \sum_x \sum_y P(x)P(y)log(\frac{1}{P(x)P(y)}) \\
                    &= \sum_x \sum_y P(x)P(y)(log(\frac{1}{P(x)}) + log(\frac{1}{P(y)})) \\
                    &= \sum_x P(x)log(\frac{1}{P(x)})\sum_y P(y) + \sum_y P(y)log(\frac{1}{P(y)})\sum_x P(x) \\
                    &= H(X) + H(Y)
        \end{aligned}$$ So, the logarithm does lend the information
    measure a few tricks.

# The Weighing Problem

## Problem Statement

::: displayquote
*You are given 12 balls, all equal in weight except for one that is
either heavier or lighter. You are also given a two-pan balance to use.
In each use of the balance you may put any number of the 12 balls on the
left pan, and the same number on the right pan, and push a button to
initiate the weighing; there are three possible outcomes: either the
weights are equal, or the balls on the left are heavier, or the balls on
the left are lighter. Your task is to design a strategy to determine
which is the odd ball and whether it is heavier or lighter than the
others in as few uses of the balance as possible.*
:::

*Fun fact*: This problem was featured on Brooklyn Nine-Nine and I had
been trying to solve it for a while. I found the solution! Who knew it
was in information theory? The form of the problem is slightly
different. It's islanders instead of balls. In case you're wondering
which episode this was, it was *S02E18*.\
The key to solving this problem is realising that the information gain
is maximised when there are as many equiprobable outcomes as possible.\
Don't get it? Don't worry, here the math:

We have, $$\sum_x P(x)log(\frac{1}{P(x)}) = H(X)$$

From Jensen's inequality, we know: $$E[f(x)] \leq f(E[x])$$ If we take
$f(x)=log(x)$ $$\begin{aligned}
    E[f(\frac{1}{P(x)})] & \leq f(E[\frac{1}{P(x)}]) \\
    \sum_x P(x)log(\frac{1}{P(x)}) & \leq f(|A_x|) \\
    H(X) & \leq log(|A_x|)\end{aligned}$$

The equality holds only when all the outcomes are equiprobable.\
So, the *best* strategy is dividing it up into equiprobable outcomes.
Even in Brooklyn Nine Nine, when Amy started with *\"we weight six
against six\...\"* Holt immediately stopped her and told her that it
would never work.\
Here's a description of the optimal strategy in case the diagram seems
confusing:

1.  Weigh 1234 against 5678 and leave out 4 balls. This divides it into
    three equiprobable outcomes. The first outcome is when the left is
    heavier, the second one is when the right is heavier and the third
    one is when they are equal in weight. (Think of the weighing as a
    sieve and the hypotheses as impurities or whatever.)

2.  Now, we weigh three against three. I'll explain for the first
    branch. Why do we choose 126 against 345? Actually, we could have
    chosen 125 against 346 *or* 127 against 348. If you can find any
    other weighing scheme that creates three equiprobable outcomes, it's
    fine.

3.  Then in the third weighing, we weigh one against one. The star
    denotes an impossible outcome.

Now, what gives me the right to call my strategy *optimal*? Well, we
have to refine the original hypothesis space into a single hypothesis.
If we weigh twice, then the final number of branches is $3^2=9$ which is
less than 24. Three weighings creates $3^3=27$ branches which is a
little over 24. So, it's theoretically impossible to find the right
hypothesis in less than three weighings if we assume that we're dividing
it into 3 equiprobable outcomes at every step. If you can find a
strategy with 4 equiprobable outcomes per step, shoot me an email.\

# The Theoretical Limits of Compression

## Raw Bit Content

$$H_0 (x) = log_2 |A_x|$$

The raw bit content gives a lower bound on the number od *questions*
needed to find a binary answer. Again, the log comes in handy. Say we
have an ordered pair $(x,y)$ with $|A_x||A_y|$ outcomes,

$$\begin{aligned}
    H_0 (x,y) & = log_2 |A_x||A_y| \\
            &= log_2 |A_x| + log_2 |A_y| \\
            &= H_0 (x) + H_0 (y)    \end{aligned}$$

## A Simple Problem

::: displayquote
*Could $\exists$ a compressor $x \to c(x)$ and a decompressor
$c(x) \to x$ such that every possible outcome is compressed into a
binary code shorter than $H_0 (x)$ bits?*
:::

The answer is nope. Not without introducing a probabilistic element. We
can't guarantee $c(x) \to x$ without $H_0 (x)$ bits. The word
*guarantee* here is key. If we don't want a guarantee and we'd like to
roll the dice to save space, we can do that. In fact, we do that a lot.
Intuitively, it seems like that would never work but I don't see the
world falling apart. Every now and then there's this guy who says he has
created an *ideal* compressor that can compress all files. I think this
guy hangs out with the guy who says that he has created a *perpetual
motion device* (Clearly, he did not study thermodynamics).


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Distill is a publication format for scientific and technical writing, native to the web.

Learn more about using Distill at <https://rstudio.github.io/distill>.


