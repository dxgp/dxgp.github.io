---
title: "Reading List 2021"
description: |
  Just a list of books that I plan on finishing by the end of 2021.
author:
  - name: Gunjan Payal
    url: {}
date: 03-21-2021
output:
  distill::distill_article:
    self_contained: false
draft: false
---

## Mathematics

I've spent a long time thinking about whether math is necessary for machine learning. Here's my answer: *absolutely*. Here's the most popular analogy I've heard:

> It's just like a car. To drive it, I don't have to know how the parts work.

That's just stupid. You're not driving the car in this scenario. You're the one who's fucking designing it. To design it, you can get away with not knowing the laws of thermodynamics but not without knowing how the parts fit together. There is an exception to this, though. If you wanna design cars that look like this:

![](https://media.wired.com/photos/593252a2a31264584499416b/master/w_1600%252Cc_limit/the-homer-inline2.jpg){width="150"}

If you actually want to apply machine learning to problems that are genuinely interesting and make a contribution, you have to know the math and the underlying theory. You have to know the limits of machine learning. Contrary to popular belief, machine learning is not ACTUALLY magic. Neural networks are not the solution to everything. It's problem dependent. This is from the point of view of a guy who wants to go into research. If that's not your goal, you can get away with knowing a lot less math than this.

> Unless you're comfortable with the math, you're going to crap your pants while reading research papers. Also, math's applicable EVERYWHERE. So, you don't really lose anything.

For a demonstration of crapping your pants, take a gander at this paper[^1]. If you can understand the math in there, you're good to go. Here's a little preview:

[^1]: <https://arxiv.org/abs/2007.10412>

$${\sum_{t=1}^T  \mathcal S_{P^{\phi_t}} \left[\frac{\partial L}{\partial \phi_{t}}\right] \sum_{i=1}^{t} \big(\prod_{j = i}^{t-1} \frac{\partial \phi_{j+1}}{\partial \phi_j}\big) \mathcal S_{P^{\phi_{i-1}}} \left[\frac{\partial \phi_i}  {\partial  C_{i-1}}\right]\frac{\partial  C_{i-1}}{\partial \theta}\,.}$$

Intimidating, right?

I'll conclude this rant with my math reading list for 2021:

-   Analysis I and II by Terence Tao
-   Information Theory, Inference, and Learning Algorithms by David MacKay
-   Visual Complex Analysis by Tristan Needham
-   TAOCP - Vol. 1 and 2 (Yeah, it's mostly math so I'll put it here)

<aside>

Why TAOCP? Because I either play the game on hard mode or not at all.

</aside>

For the last book, there's a great accompanying course on YT. I'll add the link to it in a few days. This year is going to be math heavy and that's a bit of a problem because people usually only look at your *projects* and I don't really have projects yet. I've tried making a CV a few times but I come up empty. Hopefully, it won't be empty by the end of this year.

I did consider Rudin for Analysis but I liked Tao's style better.

## Machine Learning

I do need to apply all that math. Here's my machine learning reading list. I should warn you that this is fairly intermediate because I read the basic stuff about a year ago.

-   The Elements of Statistical Learning by Hastie et al.
-   Deep Learning by Ian Goodfellow
-   Pattern Recognition and Machine Learning by Bishop
-   Reinforcement Learning by Sutton and Barto

## Other Books that I Think Might be Worth Reading

Sometimes, I just can't resist the temptation to dive into a completely different field. I have the attention span of a 10 year old. That said, here's the books that I can't wait to read:

-   Programming Quantum Computers by Johnston
-   Nonlinear Dynamics and Chaos by Strogatz

## Concluding Remarks

My reading list is fairly small. That's because we have vastly different courses that are currently going on in our college and I do have assignments to complete and exams to take.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
